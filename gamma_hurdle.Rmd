---
title: "Sunfish data with Gamma Hurdle Models"
output:
  pdf_document: default
date: "`r format(Sys.time(), '%d %B, %Y')`"
author: Olga Lyashevska
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12,
                      fig.height=8,
                      fig.path='figs/',
                      echo=FALSE,
                      warning=FALSE,
                      message=FALSE,
                      cache=FALSE)
```

## Research questions:

1. Has the number of sunfish sighted increased over time?
2. Can observed change be related to changes in wind speed or direction, food availability and sea surface temperature?

## Data description:
Cape Clear Data 1971-2017

Explain here how data was collected and which parts of the data are reliable.

## Variables:

```{r}

# clean up and set seed
rm(list=ls())
set.seed(4322)

```

```{r}

# load packages
library(R.utils)
library(ggplot2)
library(GGally)
library(lmtest)
library(plyr)
library(tidyverse)
library(lme4)
library(effects)
library(optimx)

```

```{r}
dat <- read.csv('~/Documents/nuig-doyle/data/sunfish/sunfish-ext2017.csv')

plot(dat$year, dat$nsunfishm, pch=20, col="blue", xlab="year",ylab="number of sunfish per minute", xlim=c(1970, 2020))

```

## Annual means for phyto and zooplankton

To obtain annual values data will be aggregated as following:
- Average for L4 data (`totzoo`, `totmedus`, `siphL4`, `cfinL4`, `chelL4`, `lcopL4`) and CPR data (`siphcpr`, `cnidarcpr`). CPR data was originally recorded as presence/absence but it has been standardised for effort;
- Median for phytoplankton data (`phytocpr`);

```{r}

# aggregated data
dat.agg <- dat %>%
  group_by(year) %>%
  summarise(totzooL4 = mean(totzooL4, na.rm=TRUE),
            totmedusL4 = mean(totmedusL4, na.rm=TRUE),
            siphL4 = mean(siphL4, na.rm=TRUE),
            cfinL4 = mean(cfinL4, na.rm=TRUE),
            chelL4 = mean(chelL4, na.rm=TRUE),
            lcopL4 = mean(lcopL4, na.rm=TRUE),
            siphcpr = mean(siphcpr, na.rm=TRUE),
            cnidarcpr = mean(cnidarcpr, na.rm=TRUE),
            phytocpr = median(as.numeric(as.character(phytocpr)), na.rm=TRUE))

# drop column
rem<-c('totzooL4', 'totmedusL4', 'siphL4', 'cfinL4', 'chelL4', 'lcopL4', 'siphcpr', 'cnidarcpr', 'phytocpr')
dat<-dat %>% select(-one_of(rem))

# join in
dat <- left_join(dat, dat.agg, by = 'year')

# drop redundant
# test <- test[, -grep(".x", colnames(test))]

# save data with annual aggregation
write.csv(dat, file='~/Documents/nuig-doyle/data/sunfish/sunfish-ext2017-agg.csv')

```
## Summary statistics for all variables

```{r}
summary(dat)

```

## Capping of outliers


```{r}

# For missing values that lie outside the 1.5 IQR limits, we cap it by replacing those observations outside the lower limit with the value of 5th percentile and those that lie above the upper limit, with the value of 95th percentile.
## technically all values fall into 1.5 IQR
#x <- dat$totmedus
#qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
#caps <- quantile(x, probs=c(.05, .95), na.rm = T)
#H <- 1.5 * IQR(x, na.rm = T)
#x[x < (qnt[1] - H)] <- caps[1]
#x[x > (qnt[2] + H)] <- caps[2]
#dat$totmedus<-x

# we replace manually values above 500 with NA# we replace manually values above 500 with NA# we replace manually values above 500 with NA

dat$totmedusL4[dat$totmedusL4>500]<-NA

```

## Model description

Dependent variable, a number of sunfish observed per minute (nsunfishm) is semi-continuous (i.e. a point mass in a single value and a continuous distribution elsewhere). The data generating process for this type of data can be modelled using a gamma distribution.  The main problem is however that response variable has a high proportion of zeros (`r round(100 - (nrow(dat[dat$nsunfishm!=0,])/nrow(dat))*100, 2)`%), which is more than expected from a gamma distribution with, therefore it cannot be readily applied.

Lets consider the two common methods for dealing with zero-inflated data:

(1) Modelling a zero-inflation parameter that represents the probability a given 0 comes from the main distribution (say the negative binomial distribution) or is an excess 0;
(2) Modelling the zero and non-zero data with one model and then modelling the non-zero data with another. This is often called a hurdle model.

In (1), the response variable is modelled as a mixture of a Bernoulli distribution (a point mass at zero) and a Poisson distribution (or any other count distribution supported on non-negative integers).
In (2), the basic idea is that a Bernoulli probability governs the binary outcome of whether a variable has a zero or positive realization. If the realization is positive, the hurdle is crossed, and the conditional distribution of the positives is governed by a truncated-at-zero model. Hurdle models model the zeros and non-zeros as two separate processes and can be useful in that they allow you to model the zeros and non-zeros with different predictors or different roles of the same predictors.

Zero-inflation models may be more elegant and informative if the same predictors are thought to contribute to the extra and real zeros.

Hurdle models can be useful in that they allow you to model the zeros and non-zeros with different predictors or different roles of the same predictors. Maybe one process leads to the zero/non-zero data and another leads to the non-zero magnitude.

Here we shall focus on (2) and model the zeros separately from the non-zeros in a binomial-Gamma hurdle model.

## Data transformation

### Scale

```{r}
# select variables to scale
cols = c("minute", "nobserver", "shark", "leatherback", "sst", "sstanom", "totzooL4", "totmedusL4", "siphL4", "cfinL4", "chelL4", "lcopL4", "siphcpr", "cnidarcpr", "iso13")
# scale variables and add to a df
dat[, paste0(cols, "_", "sc")] <- scale(dat[ ,cols])
summary(dat)

# remove ferry = 1
# zero effort positive entries

dat = dat[dat$ferry==0,]

```

### Multicollinearity check

If the explanatory variables are perfectly correlated parameters the model becomes indeterminate and standard errors of the estimates become infinitely large. When two or more of the explanatory variables are approximately linearly related regression estimators are still unbiased, but they have large variances and covariances, making precise estimation difficult. As a result, the confidence intervals tend to be wider. Therefore, we may not reject the “zero null hypothesis” (i.e. the true population coefficient is zero). The estimators and their standard errors can be sensitive to small changes in the data.

We shall examine the correlation between each pair of explanatory variables (grouped).

```{r}
cols = c("sstanom", "iso13")
ggpairs(dat[,cols])

```
Variables `iso13` and `sstanom` are correlated, therefore we should consider removing one of them from further analysis. Variable `sst` will not be included into analysis, since it has a seasonal effect.

Food availability variables:

```{r}
cols = c("totzooL4", "totmedusL4", "siphL4", "cfinL4", "chelL4", "lcopL4", "siphcpr","cnidarcpr")
ggpairs(dat[,cols])

```

Variables `lcopL4` and `chelL4` are highly correlated (0.95), one of them should be removed from further analysis.
Variables `cnidarcpr` and `siphcpr` are correlated too (0.84)

All food variables aggregated on year level to avoid any seasonality.

## Fitting full model
## Binomial model

When relating the sightings to zooplankton abundance and temperature what we are interested in detecting are annual trends over and above seasonal fluctuations that we would expect. So we would expect that within each year as temperature increases during spring and summer and zooplankton blooms occur, sunfish sightings will increase. What we want to know is - in a year when zooplankton abundance and temperatures are high are sunfish sightings also high. So we need to incorporate some sort of month effect here. Either by including month as an explanatory variable or maybe by expressing each monthly temperature and zooplankton measurement as the difference between that measurement and the monthly mean for the time series. There is a significant effect of temperature and zooplankton abundance but this may just reflect seasonal rather than annual patterns.

```{r, cache=FALSE}

summary(glm(non_zero ~
            minute_sc +
            month +
            year +
            nobserver_sc +
            wdirlocal2 +
            seastate +
            shark_sc +
            leatherback_sc +
            phytocpr +
            siphcpr_sc +
            iso13_sc,
        data = dat,
        family = binomial(link = logit)))

```
We see that observer related variables are highly significant. We try to isolate their effect for each year.
We apply a mixed-effect modeling framework and fit a varying intercept model with lmer. This approach is useful when we are interested explicitly in variation among and by groups. Group level variables are specified using a special syntax: (1|year) to fit a linear model with a varying-intercept group effect using the variable year.


```{r}
# year as random effect with noise variables
# https://stats.stackexchange.com/questions/242109/model-failed-to-converge-warning-in-lmer

m.bin.full.re <- glmer(non_zero ~
             minute_sc +
             month +
             nobserver_sc +
             wdirlocal2 +
             seastate +
             (1|year) ,
           data = dat,
           control = glmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')),
           family = binomial(link = logit))

# fit model with year as fixed effect, to compare models and to see whether year as RE gives a better fit
m.bin.full.nore <- glm(non_zero ~
             minute_sc +
             month +
             nobserver_sc +
             wdirlocal2 +
             seastate,  
             # year,
           data = dat,
           family = binomial(link = logit))

# compare models
anova(m.bin.full.re, m.bin.full.nore)

# model with random effect has lower AIC and BIC, hence better

summary(m.bin.full.re)

summary(m.bin.full.nore)

```

Random effects are conditional modes - the difference between the average predicted response for a given set of fixed-effect values (observer related variables) and the response predicted for particular year.

```{r}

# extract random effect
ranef.bin.dat<-as.data.frame(ranef(m.bin.full.re))[c(3,4)]

# rename
colnames(ranef.bin.dat)[2] <- "reyear"
# merge with dat by column year
dat.bin.re<-merge(dat,ranef.bin.dat, by.x = "year", by.y = "grp")

# fit rest of the variables against ranef.year
m.bin.full <- glm(reyear ~
             shark_sc +
             leatherback_sc +
             phytocpr +
             siphcpr_sc +
             iso13_sc ,
           data = dat.bin.re,
           family = gaussian)

summary(m.bin.full)


write.csv(dat.bin.re,'~/Documents/nuig-doyle/data/sunfish/dat_bin_re.csv')

```
## Model interpretation

## The mean probability of sighting a sunfish (random effect) against year


## The model predicted RE for each year, assuming all other model variables remain constant

```{r, echo=T}
# https://socialsciences.mcmaster.ca/jfox/Courses/R/ICPSR/jss2627.pdf

plot(allEffects(m.bin.full.re))
plot(allEffects(m.bin.full))

# save individual varibales
write.csv(predictorEffect('iso13_sc', m.bin.full, focal.levels=50, xlevels = length(unique(dat.bin.re$reyear))),'~/Documents/nuig-doyle/data/sunfish/predictor_effect_bin_iso13_sc.csv')

write.csv(predictorEffect('siphcpr_sc', m.bin.full, focal.levels=50, xlevels = length(unique(dat.bin.re$reyear))),'~/Documents/nuig-doyle/data/sunfish/predictor_effect_bin_siphcpr_sc.csv')

write.csv(predictorEffect('phytocpr', m.bin.full, focal.levels=50, xlevels = length(unique(dat.bin.re$reyear))),'~/Documents/nuig-doyle/data/sunfish/predictor_effect_bin_phytocpr.csv')


```



## Gamma model

We fit Gamma model with log link using a similar set of predictors.

```{r, cache=FALSE}

# drop rows with nas
no.na.dat <- na.omit(dat)

summary(glm(nsunfishm ~
            year+
            month +
            minute +
            nobserver +
            wdirlocal2 +
            seastate +
            phytocpr +
            shark +
            leatherback +
            siphcpr +
            iso13 +
            ferry,
            data = subset(no.na.dat, non_zero == 1),
            family = Gamma(link = log)))

```





We consider only positive observations (99) and separate observer related effect for each year.

```{r}

# nrow(subset(no.na.dat, non_zero == 1))

m.gamma.full.re <- glmer(nsunfishm ~
             minute_sc +
             month +
             nobserver_sc +
             wdirlocal2 +
             seastate +
             (1|year),
           data = subset(no.na.dat, non_zero == 1),
           control = glmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')),
           family = Gamma(link = log))

m.gamma.full.nore <- glm(nsunfishm ~
             minute_sc +
             month +
             nobserver_sc +
             wdirlocal2 +
             seastate,
             # year,
           data = subset(no.na.dat, non_zero == 1),
           family = Gamma(link = log))

# compare models
anova(m.gamma.full.re, m.gamma.full.nore)
# model with random effect has slightly lower AIC and BIC, hence better

summary(m.gamma.full.nore)
summary(m.gamma.full.re)


```



```{r}
# extract random effect
ranef.gamma.dat<-as.data.frame(ranef(m.gamma.full.re))[c(3,4)]

# rename
colnames(ranef.gamma.dat)[2] <- "reyear"
# merge with dat by column year
dat.gamma.re <- merge(subset(no.na.dat, non_zero == 1),ranef.gamma.dat, by.x = "year", by.y = "grp")

# fit the rest of the variables against ranef.year
m.gamma.full <- glm(reyear ~
             shark_sc +
             leatherback_sc +
             phytocpr +
             siphcpr_sc +
             iso13_sc,
           data = dat.gamma.re,
           family = gaussian)

summary(m.gamma.full)

write.csv(dat.gamma.re,'~/Documents/nuig-doyle/data/sunfish/dat_gamma_re.csv')
```

```{r, echo=T}
# https://socialsciences.mcmaster.ca/jfox/Courses/R/ICPSR/jss2627.pdf

plot(allEffects(m.gamma.full.re))

plot(allEffects(m.gamma.full))

# save individual varibales

write.csv(predictorEffect('iso13_sc', m.gamma.full, focal.levels=50, xlevels = length(unique(dat.gamma.re$reyear))),'~/Documents/nuig-doyle/data/sunfish/predictor_effect_gamma_iso13_sc.csv')

write.csv(predictorEffect('siphcpr_sc', m.gamma.full, focal.levels=50, xlevels = length(unique(dat.gamma.re$reyear))),'~/Documents/nuig-doyle/data/sunfish/predictor_effect_gamma_siphcpr_sc.csv')

write.csv(predictorEffect('phytocpr', m.gamma.full, focal.levels=50, xlevels = length(unique(dat.gamma.re$reyear)))
,'~/Documents/nuig-doyle/data/sunfish/predictor_effect_gamma_phytocpr.csv')

```


This 2-part model specifies the probability of seeing a sunfish and the number of sunfish being sighted conditional on a response being positive, thereby indirectly specifying a different parametrization for the overall marginal means. Although this model allows to consider different covariates or different meaning of the same covariates, we have limitations for estimation of covariate effects on the overall mean.  On the other hand, standard one-part models are assumed to come from the same data-generation process and there are 2 processes that can produce zeros.

All interpretations were conducted with covariates on the transformed scale. 

## References

* https://www.ssc.wisc.edu/sscc/pubs/MM/MM_DiagInfer.html
* http://rstudio-pubs-static.s3.amazonaws.com/5691_192685385fc445c9b3fb1619960a20e2.html
* http://pj.freefaculty.org/guides/stat/Regression-GLM/Gamma/GammaGLM-01.pdf
* https://stats.stackexchange.com/questions/81457/what-is-the-difference-between-zero-inflated-and-hurdle-distributions-models
* http://seananderson.ca/2014/05/18/gamma-hurdle.html
* https://ms.mcmaster.ca/~bolker/R/misc/modelDiag.html
* https://stats.idre.ucla.edu/r/dae/logit-regression/
* https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/
* http://environmentalcomputing.net/interpreting-coefficients-in-glms/
* https://www.sciencedirect.com/science/article/pii/S0167947308000169
* https://www.cambridge.org/core/services/aop-cambridge-core/content/view/S0021859611000608
* https://link.springer.com/article/10.1007/s10742-017-0169-9
* https://stats.stackexchange.com/questions/96972/how-to-interpret-parameters-in-glm-with-family-gamma/126225
* https://stats.stackexchange.com/questions/161216/backtransform-coefficients-of-a-gamma-log-glmm
